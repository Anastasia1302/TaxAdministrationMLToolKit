{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1\n",
    "\n",
    "## A. What is text?\n",
    "\n",
    "* Sequence of words, characters, phrases, documents.\n",
    "* Words: separated by spaces\n",
    "\n",
    "\n",
    "## B. tokenization: split input sequence into so-called tokens. \n",
    "* Token: useful unit of text for semantic analysis. \n",
    "* Tokenize by spaces, characters, using dot \".\".\n",
    "\n",
    "Python: nltk.tokenize.treebankwordtokenizer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "text=\"Este es un texto de prueba, el primero que vamos a ver.\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separador de espacios\n",
    "tokenizer1=nltk.tokenize.WhitespaceTokenizer()\n",
    "TokenTexto=tokenizer1.tokenize(text)\n",
    "print(TokenTexto)\n",
    "print(TokenTexto[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separador de puntuación\n",
    "tokenizer2=nltk.tokenize.TreebankWordTokenizer()\n",
    "TokenTexto2=tokenizer2.tokenize(text)\n",
    "print(TokenTexto2)\n",
    "print(TokenTexto2[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Treebank tokenization. Uses regular expressions in English. \n",
    "Texto3=\"Don't you think this isn't right?\"\n",
    "tokenizer3=nltk.tokenize.TreebankWordTokenizer()\n",
    "TokenTexto3=tokenizer2.tokenize(Texto3)\n",
    "print(TokenTexto3)\n",
    "print(TokenTexto3[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Token normalization. We might want the same token for different forms of the words. \n",
    "\n",
    "#### 1.a. Stemming (root). Using Porter’s stemmer. Replacing and removing suffixes to get to the root of the words (stem). Problem is that it fails in irregular forms. \n",
    "\n",
    "* Ponnies->Ponni\n",
    "* Cats->Cat\n",
    "* Wolves-> Wolv\n",
    "\n",
    "In Python: nltk.stem.PorterStemmer \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"feet wolves cats talked\"\n",
    "tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\" \".join(stemmer.stem(token) for token in tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 1.B Lemmatization. \"Doing things properly\". Use morphology and text structure. Using WordNet Lemmatizer. Lookup for lemmas. Problem is that not all forms are reduced. \n",
    "\n",
    "* Feet->Foot\n",
    "* Wolves-> Wolf\n",
    "* talked->talked\n",
    "\n",
    " Use nltk.stem.WordNetLemmatizer\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.WordNetLemmatizer()\n",
    "\" \".join(stemmer.lemmatize(token) for token in tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " #### General principle: try both and see which works better.  Lemmatization or steming.\n",
    "\n",
    " #### 1. C Further normalization:\n",
    "\n",
    "            Normalize capital letters. U.S->us\n",
    "            Acronyms -> ETA->eta->e.t.a.\n",
    "            Write set of rules?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 2 Feature extraction from text\n",
    "\n",
    "\n",
    "Count the occurrences of a particular token in our text. ->\"Bag of words\".\n",
    "For each token we will have a feature column. This is called text vectorization. \n",
    "Problems with text vectorization:\n",
    "* 1. We lose order, hence the name bag of words\n",
    "* 2. Counters are not normalized\n",
    "\n",
    "#### 2. A. Solving the issue of order: token pairs, triplets, etc. The problem with this-> too many features. \n",
    "#### 2.B Another. remove some n.grams. \n",
    "\n",
    "* High frequency n-grams: articles, prepositions, etc. This are stop-words-> they won’t help to discriminate the text. Ideally remove \n",
    "\n",
    "* Low frequency n-grams: typos. We don’t need them. If we don't remove them-> overfit. \n",
    "\n",
    "* Medium frequency n-grams: those are good n-grams. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.c. Measuring frequency of n.grams. Term Frequency (TF)\n",
    "\n",
    "\n",
    "* binary-> 0,1\n",
    "\n",
    "* raw count->$f_{d,t}$  Number of times term $t$ appears in document $d$. \n",
    "\n",
    "* term frequency-> $f_{d,t}/\\sum_{t' \\in d}f_{d,t'}$\n",
    "\n",
    "* log-normalization->$1+log(f_{d,t})$\n",
    "\n",
    "* Inverse document frecuency\n",
    "\n",
    "    $N=|D|:$total number of documents in copus\n",
    "    \n",
    "    $|{d\\in D : t\\in d}|$\n",
    "    \n",
    "    $idf(t,D)=log\\frac{N}{|{d\\in D : t\\in d}|}$\n",
    "    \n",
    "    \n",
    "* TF-IDF:\n",
    "\n",
    "    $tfidf(t,d,D)=tf(t,d)idf(t,D)$\n",
    "    \n",
    "    **Note that a high tfidf is reached by having high term frequency and low document frequency of the term in the    set of all the collection of documents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF example in python\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "texts = [\n",
    "    \"good movie\", \"not a good movie\", \"did not like\", \n",
    "    \"i like it\", \"good one\"\n",
    "]\n",
    "# using default tokenizer in TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1, 2))\n",
    "features = tfidf.fit_transform(texts)\n",
    "pd.DataFrame(\n",
    "    features.todense(),\n",
    "    columns=tfidf.get_feature_names()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Linear models for sentiment analysis\n",
    "\n",
    "IMDB movie review dataset. \n",
    "\n",
    "4 out of 10-> Bad\n",
    "\n",
    "7 or more out of 10-> Good\n",
    "\n",
    "Binary classification-> Good or bad review. \n",
    "\n",
    "Highly sparse. Rather than trees, we can use Bayes classification or linear models.\n",
    "\n",
    "Linear classification by logistic regression. Advantages: fast, interpretable, can handle sparse data. \n",
    "\n",
    "In general, deep learning does not improve significantly accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hashing trick in spam filtering\n",
    "\n",
    "* If your dataset is small, it is ok to have n-gram and its correspondence on tfidf.\n",
    "* If dataset is huge, problem. \n",
    "    Solution: n-gram->hash(n-gram)% $2^{20}$\n",
    "    \n",
    "    \n",
    "* Hashing example\n",
    "$\\phi(S)=s[0]+p_1s[1]+p_2s[2]+...$   $p$ are prime numbers\n",
    "\n",
    "* Hash collision-> Different strings with same hash.\n",
    "* Hash works quite ok when compared to linear models. \n",
    "\n",
    "### Vowpal Wabbit Library in Python. Uses sophisticated hashing functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def test_regex(s):\n",
    "    return re.sub('[^a-z]', '', s.lower())\n",
    "test_regex('aLgo')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Texttest='Alg4@#oSSssSS'\n",
    "A=re.sub('', '', Texttest.lower())\n",
    "print(A)\n",
    "C=\"alalAAaABBbB\"\n",
    "C.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BAD_SYMBOLS_RE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples of regular expressions\n",
    "\n",
    "https://docs.python.org/2/howto/regex.html#regex-howto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = re.compile('[a-z]+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(p.match('algo4'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(p.findall('algo423ala32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match() method only checks if the RE matches at the start of a string,\n",
    "m = p.match('tempo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Match only begining of\n",
    "n = p.match('23tempo')\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Two pattern methods return all of the matches for a pattern. findall() returns a list of matching strings:\n",
    "#\\d Matches any decimal digit; this is equivalent to the class [0-9].\n",
    "p = re.compile('\\d+')\n",
    "p.findall('12 drummers drumming, 11 pipers piping, 10 lords a-leaping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#findall() has to create the entire list before it can be returned as the result. \n",
    "#The finditer() method returns a sequence of match object instances as an iterator. [1]\n",
    "\n",
    "iterator = p.finditer('12 drummers drumming, 11 ... 10 ...')\n",
    "iterator  \n",
    "for match in iterator:\n",
    "    print(match.span())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(re.match(r'From\\s+', 'Fromage amk'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(re.match(r'From\\s+', 'From amk Thu May 14 19:12:10 1998'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BAD_SYMBOLS_RE.findall('12 drummers drumming, 11 pipers piping, 10 lords a-leaping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "re.sub(BAD_SYMBOLS_RE, \" \", \"algo,-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "#STOPWORDS = set(stopwords.words('english'))\n",
    "re.sub(REPLACE_BY_SPACE_RE, \" \", \"234@kjasdf#()()4&$*al-_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "text = re.sub(BAD_SYMBOLS_RE, \"\", \"234@kjasdf#()()4&$*al-_\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download()\n",
    "#################################################\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r'\\b(' + r'|'.join(stopwords.words('english')) + r')\\b\\s*')\n",
    "text = pattern.sub('', \"HELLO a and himself all the people that wanted to join \") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def text_prepare(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    text = text.lower() #Lowercase text \n",
    "    text = re.sub(REPLACE_BY_SPACE_RE, \" \", text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = text = re.sub(BAD_SYMBOLS_RE, \"\", text)# delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    # delete stopwords from text\n",
    "    pattern = re.compile(r'\\b(' + r'|'.join(stopwords.words('english')) + r')\\b\\s*')\n",
    "    text= pattern.sub('', text) \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_prepare(\"SQL Server - any equivalent of Excel's CHOOSE function?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_prepare(\"sql server equivalent excels choose function\")==\"sql server equivalent excels choose function\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"good movie\", \"not a good movie\", \"did not like\", \n",
    "    \"i like it\", \"good one\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "cv_fit=cv.fit_transform(texts)\n",
    "Words=cv.get_feature_names()\n",
    "print(Words)\n",
    "\n",
    "Frequencies=cv_fit.toarray().sum(axis=0)\n",
    "print(Frequencies)\n",
    "Attempt=dict(zip(Words, Frequencies))\n",
    "print(Attempt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_x = sorted(Attempt.items(), key=lambda kv: -kv[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted_x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
