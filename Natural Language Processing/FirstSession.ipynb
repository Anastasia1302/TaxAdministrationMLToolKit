{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classification wofkflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from common.download_utils import download_week1_resources\n",
    "\n",
    "download_week1_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download()\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Getting ready everything for the task. \n",
    "\n",
    "In this task you will deal with a dataset of post titles from StackOverflow. You are provided a split to 3 sets: train, validation and test. All corpora (except for test) contain titles of the posts and corresponding tags (100 tags are available). Upload the corpora using pandas and look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    data = pd.read_csv(filename, sep='\\t')\n",
    "    data['tags'] = data['tags'].apply(literal_eval)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = read_data('data/train.tsv')\n",
    "validation = read_data('data/validation.tsv')\n",
    "test = pd.read_csv('data/test.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = train['title'].values, train['tags'].values\n",
    "X_val, y_val = validation['title'].values, validation['tags'].values\n",
    "X_test = test['title'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each tag and for each word calculate how many times they occur in the train corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Dictionary of all tags from train corpus with their counts.\n",
    "tags_counts = {}\n",
    "# Dictionary of all words from train corpus with their counts.\n",
    "words_counts = {}\n",
    "\n",
    "#First doing it for tags\n",
    "import collections\n",
    "cnt = collections.Counter()\n",
    "for string in y_train:\n",
    "    for line in string:\n",
    "        cnt[line]+=1\n",
    "\n",
    "\n",
    "tags_counts = {}\n",
    "for key, value in cnt.items():\n",
    "    tags_counts[key] = value\n",
    "\n",
    "#Second, doing it for words\n",
    "cv = CountVectorizer(token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\")\n",
    "cv_fit=cv.fit_transform(X_train)\n",
    "Words=cv.get_feature_names()\n",
    "Frequencies=cv_fit.toarray().sum(axis=0)\n",
    "Attempt=dict(zip(Words, Frequencies))\n",
    "words_counts=Attempt\n",
    "words_counts2 = sorted(Attempt.items(), key=lambda kv: -kv[1])\n",
    "\n",
    "\n",
    "#Doing it with counters:\n",
    "tokenizer1=nltk.tokenize.WhitespaceTokenizer()\n",
    "TokenTexto=tokenizer1.tokenize(X_train[0])\n",
    "\n",
    "#Doing it with counters:\n",
    "cnt = collections.Counter()\n",
    "for string in X_train:\n",
    "    TokenTexto=tokenizer1.tokenize(string)\n",
    "    for string in TokenTexto:\n",
    "        cnt[string]+=1\n",
    "\n",
    "\n",
    "\n",
    "words_counts = {}\n",
    "for key, value in cnt.items():\n",
    "    words_counts[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtaining the most common words\n",
    "#Most common words\n",
    "\n",
    "most_common_words = sorted(words_counts.items(), key=lambda x: x[1], reverse=True)[:5000]\n",
    "print(most_common_words)\n",
    "\n",
    "most_commonN = {}\n",
    "indextowords={}\n",
    "key=0\n",
    "for pair in most_common_words:\n",
    "    #print(pair[0])\n",
    "    most_commonN[pair[0]] = key\n",
    "    indextowords[key]=pair[0]\n",
    "    key+=1\n",
    "#print(indextowords)\n",
    "#print(most_commonN)\n",
    "\n",
    "WORDS_TO_INDEX = most_common\n",
    "\n",
    "print(WORDS_TO_INDEX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare the text data\n",
    "\n",
    "One of the most known difficulties when working with natural data is that it's unstructured. For example, if you use it \"as is\" and extract tokens just by splitting the titles by whitespaces, you will see that there are many \"weird\" tokens like 3.5?, \"Flip, etc. To prevent the problems, it's usually useful to prepare the data somehow. In this task you'll write a function, which will be also used in the other assignments.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input necessary for the bag of words:\n",
    "DICT_SIZE = 5000\n",
    "WORDS_TO_INDEX = most_commonN\n",
    "INDEX_TO_WORDS = indextowords\n",
    "ALL_WORDS = WORDS_TO_INDEX.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def text_prepare(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    text = text.lower() #Lowercase text \n",
    "    text = re.sub(REPLACE_BY_SPACE_RE, \" \", text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text  = re.sub(BAD_SYMBOLS_RE,\"\", text)# delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    # delete stopwords from text\n",
    "    pattern = re.compile(r'\\b(' + r'|'.join(stopwords.words('english')) + r')\\b\\s*')\n",
    "    text= pattern.sub('', text) \n",
    "    text = re.sub(' +',' ',text) #Remove extra\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can preprocess the titles using function text_prepare and making sure \n",
    "#that the headers don't have bad symbols:\n",
    "\n",
    "X_train = [text_prepare(x) for x in X_train]\n",
    "X_val = [text_prepare(x) for x in X_val]\n",
    "X_test = [text_prepare(x) for x in X_test]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilabel classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer(classes=sorted(tags_counts.keys()))\n",
    "y_train = mlb.fit_transform(y_train)\n",
    "y_val = mlb.fit_transform(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Transforming text to a vector\n",
    "\n",
    "Machine Learning algorithms work with numeric data and we cannot use the provided text data \"as is\". There are many ways to transform text data to numeric vectors. In this task you will try to use two of them.\n",
    "\n",
    "#### Bag of words\n",
    "\n",
    "One of the well-known approaches is a *bag-of-words* representation. To create this transformation, follow the steps:\n",
    "1. Find *N* most popular words in train corpus and numerate them. Now we have a dictionary of the most popular words.\n",
    "2. For each title in the corpora create a zero vector with the dimension equals to *N*.\n",
    "3. For each text in the corpora iterate over words which are in the dictionary and increase by 1 the corresponding coordinate.\n",
    "\n",
    "Let's try to do it for a toy example. Imagine that we have *N* = 4 and the list of the most popular words is \n",
    "\n",
    "    ['hi', 'you', 'me', 'are']\n",
    "\n",
    "Then we need to numerate them, for example, like this: \n",
    "\n",
    "    {'hi': 0, 'you': 1, 'me': 2, 'are': 3}\n",
    "\n",
    "And we have the text, which we want to transform to the vector:\n",
    "\n",
    "    'hi how are you'\n",
    "\n",
    "For this text we create a corresponding zero vector \n",
    "\n",
    "    [0, 0, 0, 0]\n",
    "    \n",
    "And iterate over all words, and if the word is in the dictionary, we increase the value of the corresponding position in the vector:\n",
    "\n",
    "    'hi':  [1, 0, 0, 0]\n",
    "    'how': [1, 0, 0, 0] # word 'how' is not in our dictionary\n",
    "    'are': [1, 0, 0, 1]\n",
    "    'you': [1, 1, 0, 1]\n",
    "\n",
    "The resulting vector will be \n",
    "\n",
    "    [1, 1, 0, 1]\n",
    "   \n",
    "Implement the described encoding in the function *my_bag_of_words* with the size of the dictionary equals to 5000. To find the most common words use train data. You can test your code using the function *test_my_bag_of_words*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function my_bag_of_words\n",
    "\n",
    "def my_bag_of_words(text, words_to_index, dict_size):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        dict_size: size of the dictionary\n",
    "        \n",
    "        return a vector which is a bag-of-words representation of 'text'\n",
    "    \"\"\"\n",
    "    l=list()\n",
    "    tokenizer1=nltk.tokenize.WhitespaceTokenizer()\n",
    "    TokenTexto=tokenizer1.tokenize(text)\n",
    "\n",
    "    SizeN=len(TokenTexto)\n",
    "    counterP=0\n",
    "    P = [0] * len(words_to_index)\n",
    "    #List of word to index:\n",
    "\n",
    "    palabras=list(words_to_index)[0:dict_size]\n",
    "    for popular in words_to_index:\n",
    "        ct=0\n",
    "        for word in TokenTexto:\n",
    "            \n",
    "            if popular==word:\n",
    "                ct+=1\n",
    "                P[counterP]=ct\n",
    "        counterP+=1\n",
    "\n",
    "    \n",
    "    #print(P)\n",
    "    l.append(P)\n",
    "    #print(l)\n",
    "    result_vector=l\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example:\n",
    "words_to_index = {'hi': 0, 'you': 1, 'me': 2, 'are': 3}\n",
    "examples = ['hi how are you']\n",
    "answers = [[1, 1, 0, 1]]\n",
    "\n",
    "for ex, ans in zip(examples, answers):\n",
    "    print(\"ex is \"+str(ex)+\" ans is \"+str(ans))\n",
    "    print(\"my bag of words output is\"+str(my_bag_of_words(ex, words_to_index, 4)))\n",
    "    print(\"answer is.....\"+str(ans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sparse matrix\n",
    "from scipy import sparse as sp_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_mybag = sp_sparse.vstack([sp_sparse.csr_matrix(my_bag_of_words(text, WORDS_TO_INDEX, DICT_SIZE)) for text in X_train])\n",
    "X_val_mybag = sp_sparse.vstack([sp_sparse.csr_matrix(my_bag_of_words(text, WORDS_TO_INDEX, DICT_SIZE)) for text in X_val])\n",
    "X_test_mybag = sp_sparse.vstack([sp_sparse.csr_matrix(my_bag_of_words(text, WORDS_TO_INDEX, DICT_SIZE)) for text in X_test])\n",
    "print('X_train shape ', X_train_mybag.shape)\n",
    "print('X_val shape ', X_val_mybag.shape)\n",
    "print('X_test shape ', X_test_mybag.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score \n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_features(X_train, X_val, X_test):\n",
    "    \"\"\"\n",
    "        X_train, X_val, X_test — samples        \n",
    "        return TF-IDF vectorized representation of each sample and vocabulary\n",
    "    \"\"\"\n",
    "    # Create TF-IDF vectorizer with a proper parameters choice\n",
    "    # Fit the vectorizer on the train set\n",
    "    # Transform the train, test, and val sets and return the result\n",
    "    \n",
    "    \n",
    "    #tfidf_vectorizer = ####### YOUR CODE HERE ####### \\S+\n",
    "    tfidf_vectorizer = TfidfVectorizer(min_df=0.05, max_df=0.95, ngram_range=(1, 2),token_pattern='\\S+')\n",
    "    X_train=tfidf_vectorizer.fit_transform(X_train)\n",
    "    X_val=tfidf_vectorizer.transform(X_val)\n",
    "    X_test=tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "    \n",
    "    return X_train, X_val, X_test, tfidf_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the training\n",
    "def train_classifier(X_train, y_train):\n",
    "\n",
    "    #X_train_tfidf, X_val_tfidf, X_test_tfidf, tfidf_vocab = tfidf_features(X_train, X_val, X_test)\n",
    "    ovr2 = OneVsRestClassifier(LogisticRegression(penalty='l2', C=100.0, ))\n",
    "    ovr2.fit(X_train,y_train)\n",
    "    return(ovr2)\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_evaluation_scores(y_val, predicted):\n",
    "    print(accuracy_score(y_val, predicted))\n",
    "    print(f1_score(y_val, predicted, average='macro'))\n",
    "    print(f1_score(y_val, predicted, average='micro'))\n",
    "    print(\"f1 is \"+str(f1_score(y_val, predicted, average='weighted')))\n",
    "    \n",
    "    print(average_precision_score(y_val, predicted, average='macro'))\n",
    "    print(average_precision_score(y_val, predicted, average='micro'))\n",
    "    print(average_precision_score(y_val, predicted, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing the Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Doing the transformation with TFIDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf, X_val_tfidf, X_test_tfidf, tfidf_vocab = tfidf_features(X_train, X_val, X_test)\n",
    "tfidf_reversed_vocab = {i:word for word,i in tfidf_vocab.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_mybag = train_classifier(X_train_mybag, y_train)\n",
    "classifier_tfidf = train_classifier(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_predicted_labels_mybag = classifier_mybag.predict(X_val_mybag)\n",
    "y_val_predicted_scores_mybag = classifier_mybag.decision_function(X_val_mybag)\n",
    "\n",
    "y_val_predicted_labels_tfidf = classifier_tfidf.predict(X_val_tfidf)\n",
    "y_val_predicted_scores_tfidf = classifier_tfidf.decision_function(X_val_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Bag-of-words')\n",
    "print_evaluation_scores(y_val, y_val_predicted_labels_mybag)\n",
    "print('Tfidf')\n",
    "print_evaluation_scores(y_val, y_val_predicted_labels_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.  Workflow for the entire model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ModelPredictionAccuracy(X_train,X_val,X_test,X_train_mybag,X_val_mybag,X_test_mybag,\n",
    "                            y_train,y_val,penaltyListed='l2',CListed=50,min_dfListed=0.1,max_dfListed=0.9,\n",
    "                            ngram_rangeListed=(1, 2)):\n",
    "    \n",
    "    #First, obtain again the tfidf\n",
    "    \n",
    "    print(\"penalty is \"+str(penaltyListed))\n",
    "    print(\"C is \"+str(CListed))\n",
    "    print(\"max_df is \"+str(max_dfListed))\n",
    "    print(\"min_df is \"+str(min_dfListed))\n",
    "    print(\"ngram_range is \"+str(ngram_rangeListed))\n",
    "\n",
    "    \n",
    "    \n",
    "    tfidf_vectorizer = TfidfVectorizer(min_df=min_dfListed, max_df=max_dfListed, ngram_range=ngram_rangeListed,\n",
    "                                       token_pattern='\\S+')\n",
    "    \n",
    "    X_train_tfidf=tfidf_vectorizer.fit_transform(X_train)\n",
    "    X_val_tfidf=tfidf_vectorizer.transform(X_val)\n",
    "    X_test_tfidf=tfidf_vectorizer.transform(X_test)\n",
    "    \n",
    "    \n",
    "    #Second, perform logistic regression\n",
    "    Logistic_TFIDF = OneVsRestClassifier(LogisticRegression(penalty=penaltyListed, C=CListed,max_iter=500 ))\n",
    "    Logistic_MYBAG = OneVsRestClassifier(LogisticRegression(penalty=penaltyListed, C=CListed,max_iter=500 ))\n",
    "    \n",
    "    Logistic_TFIDF.fit(X_train_tfidf,y_train)\n",
    "    Logistic_MYBAG.fit(X_train_mybag,y_train)\n",
    "    \n",
    "    #Third, obtain predictions\n",
    "    y_val_predicted_labels_tfidf = Logistic_TFIDF.predict(X_val_tfidf)\n",
    "    y_val_predicted_scores_tfidf = Logistic_TFIDF.decision_function(X_val_tfidf)\n",
    "    \n",
    "    y_val_predicted_labels_mybag = Logistic_MYBAG.predict(X_val_mybag)\n",
    "    y_val_predicted_scores_mybag = Logistic_MYBAG.decision_function(X_val_mybag)\n",
    "    \n",
    "    #Fourth, measuring accuracy, f1, etc. of predictions\n",
    "    accuracy_tfidf=accuracy_score(y_val, y_val_predicted_labels_tfidf)\n",
    "    f1_tfidf=f1_score(y_val, y_val_predicted_labels_tfidf, average='weighted')\n",
    "    precision_tfidf=average_precision_score(y_val, y_val_predicted_labels_tfidf, average='weighted')\n",
    "    \n",
    "    accuracy_mybag=accuracy_score(y_val, y_val_predicted_labels_mybag)\n",
    "    f1_mybag=f1_score(y_val, y_val_predicted_labels_mybag, average='weighted')\n",
    "    precision_mybag=average_precision_score(y_val, y_val_predicted_labels_mybag, average='weighted')\n",
    "    \n",
    "    data={'Measure':['Accuracy','F1','Precision',\n",
    "                     'Accuracy','F1','Precision'],\n",
    "         'Value':[accuracy_tfidf,f1_tfidf,precision_tfidf,accuracy_mybag,f1_mybag,precision_mybag],\n",
    "         'C value':[CListed,CListed,CListed,CListed,CListed,CListed],\n",
    "         'Penalty':[penaltyListed,penaltyListed,penaltyListed,penaltyListed,penaltyListed,penaltyListed],\n",
    "         'min_df':[min_dfListed,min_dfListed,min_dfListed,min_dfListed,min_dfListed,min_dfListed],\n",
    "         'max_df':[max_dfListed,max_dfListed,max_dfListed,max_dfListed,max_dfListed,max_dfListed],\n",
    "         'ngram_range':[ngram_rangeListed,ngram_rangeListed,ngram_rangeListed,ngram_rangeListed,ngram_rangeListed,ngram_rangeListed],\n",
    "         'Model':['TFIDF','TFIDF','TFIDF','BOW','BOW','BOW']}\n",
    "        \n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    print(df)\n",
    "    return df\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Doing the lists\n",
    "\n",
    "Penalty_List=['l2','l1']\n",
    "min_df_List=[0.02,0.05,0.1]\n",
    "max_df_List=[0.6,0.7,0.8,0.9,1.0]\n",
    "ngram_range_List=[(1, 2),(1,3)]\n",
    "C_List=[1,2,5,10,20,100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Doing the loop\n",
    "it=0\n",
    "for CL in C_List:\n",
    "    for penaltyL in Penalty_List:\n",
    "        for min_dfL in min_df_List:\n",
    "            for max_dfL in max_df_List:\n",
    "                for ngram_rangeL in ngram_range_List:\n",
    "                    Results=ModelPredictionAccuracy(X_train,X_val,X_test,X_train_mybag,X_val_mybag,X_test_mybag,\n",
    "                                                     y_train,y_val,penaltyListed=penaltyL,CListed=CL,\n",
    "                                                    min_dfListed=min_dfL,max_dfListed=max_dfL,\n",
    "                                                    ngram_rangeListed=ngram_rangeL)\n",
    "                    if it==0:\n",
    "                        RESULTS2=Results\n",
    "                    if it!=0:\n",
    "                        RESULTS2=RESULTS2.append(Results)\n",
    "                    it+=1\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, obtain again the tfidf\n",
    "penaltyListed='l2'\n",
    "CListed=1\n",
    "min_dfListed=0.1\n",
    "max_dfListed=0.6\n",
    "ngram_rangeListed=(1, 2)\n",
    "\n",
    "\n",
    "for C in [1]:    \n",
    "    print(\"penalty is \"+str(penaltyListed))\n",
    "    print(\"C is \"+str(CListed))\n",
    "    print(\"max_df is \"+str(max_dfListed))\n",
    "    print(\"min_df is \"+str(min_dfListed))\n",
    "    print(\"ngram_range is \"+str(ngram_rangeListed))\n",
    "\n",
    "    \n",
    "    \n",
    "    tfidf_vectorizer = TfidfVectorizer(min_df=0.02, max_df=0.8, ngram_range=(1, 3),\n",
    "                                       token_pattern='\\S+')\n",
    "    \n",
    "    X_train_tfidf=tfidf_vectorizer.fit_transform(X_train)\n",
    "    X_val_tfidf=tfidf_vectorizer.transform(X_val)\n",
    "    X_test_tfidf=tfidf_vectorizer.transform(X_test)\n",
    "    \n",
    "    \n",
    "    #Second, perform logistic regression\n",
    "    Logistic_TFIDF = OneVsRestClassifier(LogisticRegression(penalty='l2', C=50,max_iter=500 ))\n",
    "    Logistic_MYBAG = OneVsRestClassifier(LogisticRegression(penalty='l2', C=50,max_iter=500 ))\n",
    "    \n",
    "    Logistic_TFIDF.fit(X_train_tfidf,y_train)\n",
    "    Logistic_MYBAG.fit(X_train_mybag,y_train)\n",
    "    \n",
    "    #Third, obtain predictions\n",
    "    y_val_predicted_labels_tfidf = Logistic_TFIDF.predict(X_val_tfidf)\n",
    "    y_val_predicted_scores_tfidf = Logistic_TFIDF.decision_function(X_val_tfidf)\n",
    "    \n",
    "    y_val_predicted_labels_mybag = Logistic_MYBAG.predict(X_val_mybag)\n",
    "    y_val_predicted_scores_mybag = Logistic_MYBAG.decision_function(X_val_mybag)\n",
    "    \n",
    "    #Fourth, measuring accuracy, f1, etc. of predictions\n",
    "    accuracy_tfidf=accuracy_score(y_val, y_val_predicted_labels_tfidf)\n",
    "    f1_tfidf=f1_score(y_val, y_val_predicted_labels_tfidf, average='weighted')\n",
    "    precision_tfidf=average_precision_score(y_val, y_val_predicted_labels_tfidf, average='weighted')\n",
    "    \n",
    "    accuracy_mybag=accuracy_score(y_val, y_val_predicted_labels_mybag)\n",
    "    f1_mybag=f1_score(y_val, y_val_predicted_labels_mybag, average='weighted')\n",
    "    precision_mybag=average_precision_score(y_val, y_val_predicted_labels_mybag, average='weighted')\n",
    "    \n",
    "    data={'Measure':['Accuracy','F1','Precision',\n",
    "                     'Accuracy','F1','Precision'],\n",
    "         'Value':[accuracy_tfidf,f1_tfidf,precision_tfidf,accuracy_mybag,f1_mybag,precision_mybag],\n",
    "         'C value':[CListed,CListed,CListed,CListed,CListed,CListed],\n",
    "         'Penalty':[penaltyListed,penaltyListed,penaltyListed,penaltyListed,penaltyListed,penaltyListed],\n",
    "         'min_df':[min_dfListed,min_dfListed,min_dfListed,min_dfListed,min_dfListed,min_dfListed],\n",
    "         'max_df':[max_dfListed,max_dfListed,max_dfListed,max_dfListed,max_dfListed,max_dfListed],\n",
    "         'ngram_range':[ngram_rangeListed,ngram_rangeListed,ngram_rangeListed,ngram_rangeListed,ngram_rangeListed,ngram_rangeListed],\n",
    "         'Model':['TFIDF','TFIDF','TFIDF','BOW','BOW','BOW']}\n",
    "        \n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(RESULTS2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in [1]:\n",
    "\n",
    "    tfidf_vectorizer = TfidfVectorizer(min_df=min_dfListed, max_df=max_dfListed, ngram_range=ngram_rangeListed,\n",
    "                                       token_pattern='\\S+')\n",
    "    \n",
    "    X_train_tfidf=tfidf_vectorizer.fit_transform(X_train)\n",
    "    X_val_tfidf=tfidf_vectorizer.transform(X_val)\n",
    "    X_test_tfidf=tfidf_vectorizer.transform(X_test)\n",
    "    \n",
    "    \n",
    "    #Second, perform logistic regression\n",
    "    Logistic_TFIDF = OneVsRestClassifier(LogisticRegression(penalty=penaltyListed, C=CListed,max_iter=500 ))\n",
    "    Logistic_MYBAG = OneVsRestClassifier(LogisticRegression(penalty=penaltyListed, C=CListed,max_iter=500 ))\n",
    "    \n",
    "    Logistic_TFIDF.fit(X_train_tfidf,y_train)\n",
    "    Logistic_MYBAG.fit(X_train_mybag,y_train)\n",
    "    \n",
    "    #Third, obtain predictions\n",
    "    y_val_predicted_labels_tfidf = Logistic_TFIDF.predict(X_val_tfidf)\n",
    "    y_val_predicted_scores_tfidf = Logistic_TFIDF.decision_function(X_val_tfidf)\n",
    "    \n",
    "    y_val_predicted_labels_mybag = Logistic_MYBAG.predict(X_val_mybag)\n",
    "    y_val_predicted_scores_mybag = Logistic_MYBAG.decision_function(X_val_mybag)\n",
    "    \n",
    "    #Fourth, measuring accuracy, f1, etc. of predictions\n",
    "    accuracy_tfidf=accuracy_score(y_val, y_val_predicted_labels_tfidf)\n",
    "    f1_tfidf=f1_score(y_val, y_val_predicted_labels_tfidf, average='weighted')\n",
    "    precision_tfidf=average_precision_score(y_val, y_val_predicted_labels_tfidf, average='weighted')\n",
    "    \n",
    "    accuracy_mybag=accuracy_score(y_val, y_val_predicted_labels_mybag)\n",
    "    f1_mybag=f1_score(y_val, y_val_predicted_labels_mybag, average='weighted')\n",
    "    precision_mybag=average_precision_score(y_val, y_val_predicted_labels_mybag, average='weighted')\n",
    "    \n",
    "    data={'Measure':['Accuracy','F1','Precision',\n",
    "                     'Accuracy','F1','Precision'],\n",
    "         'Value':[accuracy_tfidf,f1_tfidf,precision_tfidf,accuracy_mybag,f1_mybag,precision_mybag],\n",
    "         'C value':[CListed,CListed,CListed,CListed,CListed,CListed],\n",
    "         'Penalty':[penaltyListed,penaltyListed,penaltyListed,penaltyListed,penaltyListed,penaltyListed],\n",
    "         'min_df':[min_dfListed,min_dfListed,min_dfListed,min_dfListed,min_dfListed,min_dfListed],\n",
    "         'max_df':[max_dfListed,max_dfListed,max_dfListed,max_dfListed,max_dfListed,max_dfListed],\n",
    "         'ngram_range':[ngram_rangeListed,ngram_rangeListed,ngram_rangeListed,ngram_rangeListed,ngram_rangeListed,ngram_rangeListed],\n",
    "         'Model':['TFIDF','TFIDF','TFIDF','BOW','BOW','BOW']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in [1]:\n",
    "    Logistic_MYBAG = OneVsRestClassifier(LogisticRegression(penalty='l1', C=1,max_iter=500 ))\n",
    "    Logistic_MYBAG.fit(X_train_mybag,y_train) \n",
    "    y_val_predicted_labels_mybag = Logistic_MYBAG.predict(X_val_mybag)\n",
    "    #Fourth, measuring accuracy, f1, etc. of predictions\n",
    "    accuracy_mybag=accuracy_score(y_val, y_val_predicted_labels_mybag)\n",
    "    f1_mybag=f1_score(y_val, y_val_predicted_labels_mybag, average='weighted')\n",
    "    precision_mybag=average_precision_score(y_val, y_val_predicted_labels_mybag, average='weighted')\n",
    "    print('-----')\n",
    "    print('l1 c=50')\n",
    "    print(accuracy_mybag)\n",
    "    print(f1_mybag)\n",
    "    print(precision_mybag)\n",
    "    \n",
    "    Logistic_MYBAG = OneVsRestClassifier(LogisticRegression(penalty='l1', C=5,max_iter=500 ))\n",
    "    Logistic_MYBAG.fit(X_train_mybag,y_train) \n",
    "    y_val_predicted_labels_mybag = Logistic_MYBAG.predict(X_val_mybag)\n",
    "    #Fourth, measuring accuracy, f1, etc. of predictions\n",
    "    accuracy_mybag=accuracy_score(y_val, y_val_predicted_labels_mybag)\n",
    "    f1_mybag=f1_score(y_val, y_val_predicted_labels_mybag, average='weighted')\n",
    "    precision_mybag=average_precision_score(y_val, y_val_predicted_labels_mybag, average='weighted')\n",
    "    print('-----')\n",
    "    print('l1 c=50')\n",
    "    print(accuracy_mybag)\n",
    "    print(f1_mybag)\n",
    "    print(precision_mybag)\n",
    "    \n",
    "    Logistic_MYBAG = OneVsRestClassifier(LogisticRegression(penalty='l1', C=10,max_iter=500 ))\n",
    "    Logistic_MYBAG.fit(X_train_mybag,y_train) \n",
    "    y_val_predicted_labels_mybag = Logistic_MYBAG.predict(X_val_mybag)\n",
    "    #Fourth, measuring accuracy, f1, etc. of predictions\n",
    "    accuracy_mybag=accuracy_score(y_val, y_val_predicted_labels_mybag)\n",
    "    f1_mybag=f1_score(y_val, y_val_predicted_labels_mybag, average='weighted')\n",
    "    precision_mybag=average_precision_score(y_val, y_val_predicted_labels_mybag, average='weighted')\n",
    "    print('-----')\n",
    "    print('l1 c=50')\n",
    "    print(accuracy_mybag)\n",
    "    print(f1_mybag)\n",
    "    print(precision_mybag)\n",
    "    \n",
    "    \n",
    "    Logistic_MYBAG = OneVsRestClassifier(LogisticRegression(penalty='l1', C=20,max_iter=500 ))\n",
    "    Logistic_MYBAG.fit(X_train_mybag,y_train) \n",
    "    y_val_predicted_labels_mybag = Logistic_MYBAG.predict(X_val_mybag)\n",
    "    #Fourth, measuring accuracy, f1, etc. of predictions\n",
    "    accuracy_mybag=accuracy_score(y_val, y_val_predicted_labels_mybag)\n",
    "    f1_mybag=f1_score(y_val, y_val_predicted_labels_mybag, average='weighted')\n",
    "    precision_mybag=average_precision_score(y_val, y_val_predicted_labels_mybag, average='weighted')\n",
    "    print('-----')\n",
    "    print('l1 c=50')\n",
    "    print(accuracy_mybag)\n",
    "    print(f1_mybag)\n",
    "    print(precision_mybag)\n",
    "    \n",
    "    \n",
    "    Logistic_MYBAG = OneVsRestClassifier(LogisticRegression(penalty='l1', C=50,max_iter=500 ))\n",
    "    Logistic_MYBAG.fit(X_train_mybag,y_train) \n",
    "    y_val_predicted_labels_mybag = Logistic_MYBAG.predict(X_val_mybag)\n",
    "    #Fourth, measuring accuracy, f1, etc. of predictions\n",
    "    accuracy_mybag=accuracy_score(y_val, y_val_predicted_labels_mybag)\n",
    "    f1_mybag=f1_score(y_val, y_val_predicted_labels_mybag, average='weighted')\n",
    "    precision_mybag=average_precision_score(y_val, y_val_predicted_labels_mybag, average='weighted')\n",
    "    print('-----')\n",
    "    print('l1 c=50')\n",
    "    print(accuracy_mybag)\n",
    "    print(f1_mybag)\n",
    "    print(precision_mybag)\n",
    "    \n",
    "    \n",
    "    Logistic_MYBAG = OneVsRestClassifier(LogisticRegression(penalty='l1', C=100,max_iter=500 ))\n",
    "    Logistic_MYBAG.fit(X_train_mybag,y_train) \n",
    "    y_val_predicted_labels_mybag = Logistic_MYBAG.predict(X_val_mybag)\n",
    "    #Fourth, measuring accuracy, f1, etc. of predictions\n",
    "    accuracy_mybag=accuracy_score(y_val, y_val_predicted_labels_mybag)\n",
    "    f1_mybag=f1_score(y_val, y_val_predicted_labels_mybag, average='weighted')\n",
    "    precision_mybag=average_precision_score(y_val, y_val_predicted_labels_mybag, average='weighted')\n",
    "    print('-----')\n",
    "    print('l1 c=50')\n",
    "    print(accuracy_mybag)\n",
    "    print(f1_mybag)\n",
    "    print(precision_mybag)\n",
    "    \n",
    "    \n",
    "    Logistic_MYBAG = OneVsRestClassifier(LogisticRegression(penalty='l2', C=1,max_iter=500 ))\n",
    "    Logistic_MYBAG.fit(X_train_mybag,y_train) \n",
    "    y_val_predicted_labels_mybag = Logistic_MYBAG.predict(X_val_mybag)\n",
    "    #Fourth, measuring accuracy, f1, etc. of predictions\n",
    "    accuracy_mybag=accuracy_score(y_val, y_val_predicted_labels_mybag)\n",
    "    f1_mybag=f1_score(y_val, y_val_predicted_labels_mybag, average='weighted')\n",
    "    precision_mybag=average_precision_score(y_val, y_val_predicted_labels_mybag, average='weighted')\n",
    "    print('-----')\n",
    "    print('l1 c=50')\n",
    "    print(accuracy_mybag)\n",
    "    print(f1_mybag)\n",
    "    print(precision_mybag)\n",
    "    \n",
    "    Logistic_MYBAG = OneVsRestClassifier(LogisticRegression(penalty='l2', C=5,max_iter=500 ))\n",
    "    Logistic_MYBAG.fit(X_train_mybag,y_train) \n",
    "    y_val_predicted_labels_mybag = Logistic_MYBAG.predict(X_val_mybag)\n",
    "    #Fourth, measuring accuracy, f1, etc. of predictions\n",
    "    accuracy_mybag=accuracy_score(y_val, y_val_predicted_labels_mybag)\n",
    "    f1_mybag=f1_score(y_val, y_val_predicted_labels_mybag, average='weighted')\n",
    "    precision_mybag=average_precision_score(y_val, y_val_predicted_labels_mybag, average='weighted')\n",
    "    print('-----')\n",
    "    print('l1 c=50')\n",
    "    print(accuracy_mybag)\n",
    "    print(f1_mybag)\n",
    "    print(precision_mybag)\n",
    "    \n",
    "    Logistic_MYBAG = OneVsRestClassifier(LogisticRegression(penalty='l2', C=10,max_iter=500 ))\n",
    "    Logistic_MYBAG.fit(X_train_mybag,y_train) \n",
    "    y_val_predicted_labels_mybag = Logistic_MYBAG.predict(X_val_mybag)\n",
    "    #Fourth, measuring accuracy, f1, etc. of predictions\n",
    "    accuracy_mybag=accuracy_score(y_val, y_val_predicted_labels_mybag)\n",
    "    f1_mybag=f1_score(y_val, y_val_predicted_labels_mybag, average='weighted')\n",
    "    precision_mybag=average_precision_score(y_val, y_val_predicted_labels_mybag, average='weighted')\n",
    "    print('-----')\n",
    "    print('l1 c=50')\n",
    "    print(accuracy_mybag)\n",
    "    print(f1_mybag)\n",
    "    print(precision_mybag)\n",
    "    \n",
    "    \n",
    "    Logistic_MYBAG = OneVsRestClassifier(LogisticRegression(penalty='l2', C=20,max_iter=500 ))\n",
    "    Logistic_MYBAG.fit(X_train_mybag,y_train) \n",
    "    y_val_predicted_labels_mybag = Logistic_MYBAG.predict(X_val_mybag)\n",
    "    #Fourth, measuring accuracy, f1, etc. of predictions\n",
    "    accuracy_mybag=accuracy_score(y_val, y_val_predicted_labels_mybag)\n",
    "    f1_mybag=f1_score(y_val, y_val_predicted_labels_mybag, average='weighted')\n",
    "    precision_mybag=average_precision_score(y_val, y_val_predicted_labels_mybag, average='weighted')\n",
    "    print('-----')\n",
    "    print('l1 c=50')\n",
    "    print(accuracy_mybag)\n",
    "    print(f1_mybag)\n",
    "    print(precision_mybag)\n",
    "    \n",
    "    \n",
    "    Logistic_MYBAG = OneVsRestClassifier(LogisticRegression(penalty='l2', C=50,max_iter=500 ))\n",
    "    Logistic_MYBAG.fit(X_train_mybag,y_train) \n",
    "    y_val_predicted_labels_mybag = Logistic_MYBAG.predict(X_val_mybag)\n",
    "    #Fourth, measuring accuracy, f1, etc. of predictions\n",
    "    accuracy_mybag=accuracy_score(y_val, y_val_predicted_labels_mybag)\n",
    "    f1_mybag=f1_score(y_val, y_val_predicted_labels_mybag, average='weighted')\n",
    "    precision_mybag=average_precision_score(y_val, y_val_predicted_labels_mybag, average='weighted')\n",
    "    print('-----')\n",
    "    print('l1 c=50')\n",
    "    print(accuracy_mybag)\n",
    "    print(f1_mybag)\n",
    "    print(precision_mybag)\n",
    "    \n",
    "    \n",
    "    Logistic_MYBAG = OneVsRestClassifier(LogisticRegression(penalty='l2', C=100,max_iter=500 ))\n",
    "    Logistic_MYBAG.fit(X_train_mybag,y_train) \n",
    "    y_val_predicted_labels_mybag = Logistic_MYBAG.predict(X_val_mybag)\n",
    "    #Fourth, measuring accuracy, f1, etc. of predictions\n",
    "    accuracy_mybag=accuracy_score(y_val, y_val_predicted_labels_mybag)\n",
    "    f1_mybag=f1_score(y_val, y_val_predicted_labels_mybag, average='weighted')\n",
    "    precision_mybag=average_precision_score(y_val, y_val_predicted_labels_mybag, average='weighted')\n",
    "    print('-----')\n",
    "    print('l1 c=50')\n",
    "    print(accuracy_mybag)\n",
    "    print(f1_mybag)\n",
    "    print(precision_mybag)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
