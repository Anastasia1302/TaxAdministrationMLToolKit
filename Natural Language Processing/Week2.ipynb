{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Hidden Markov Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a set of tokens, the idea is to predict a label. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x=x_1,x_2,...x_T$ is a sequence of words\n",
    "\n",
    "$y=y_1,y_2,...y_T$ is a sequence of their tags (labels)\n",
    "\n",
    "We need to find the most probable sequence of tags given the sentence:\n",
    "\n",
    "$y=\\arg \\max p(y|x)=\\arg \\max p(y,x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p(y,x)=p(x|y)p(y)\\approx\\prod_{t=1}^Tp(x_t|y_t)p(y_t|y_{t-1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a consequence of the assumptions:\n",
    "\n",
    "1. Markov assumption:  $p(y)\\approx \\prod_{t=1}^Tp(y_t|y_{t-1})$\n",
    "2. Output independence: $p(x|y)\\approx \\prod_{t=1}^T p(x_t|y_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifications of Markov Hidden Model\n",
    "\n",
    "1. Set $S=s_1,...s_N$ of hidden states\n",
    "\n",
    "2. The start state $s_0$\n",
    "\n",
    "3. Matrix $A$ of transition probabilities: $a_{ij}=p(s_j|s_i)$\n",
    "\n",
    "4. The set $O$ of visible outcomes. \n",
    "\n",
    "5. Matrix B of output probabilities: $b_{ij}=p(o_k|s_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In the case of Supervised Machine Learning-> Maximum likelihood\n",
    " \n",
    " $a_{ij}=p(s_j|s_i)=\\frac{\\sum_{t=1}^T\\left[y_{t-1}=s_i,y+t=s_j\\right]}{\\sum_{t=1}^T[y_t=s_i]}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we do not observe the labels, how can we trainthe model? We cannot obtain the indicator function. Now, let us compute probabilities rather than indicators:\n",
    "\n",
    " $a_{ij}=p(s_j|s_i)=\\frac{\\sum_{t=1}^Tp(y_{t-1}=s_i,y_t=s_j)}{\\sum_{t=1}^Tp(y_t=s_i)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baum Welch algorithm. \n",
    "\n",
    "1. E-step. \n",
    "\n",
    "Estimate posterior probabilities of hidden variables: $p(y_{t-1}=s_i,y_t=s_{j})$ This can be done effectively with dynamic programming. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. M-step. \n",
    "\n",
    "Maximum likelihood update of parameters. \n",
    "\n",
    "$a_{ij}=p(s_j|s_i)=\\frac{\\sum_{t=1}^Tp(y_{t-1}=s_i,y_t=s_j)}{\\sum_{t=1}^Tp(y_t=s_i)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Viterbi Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoding probabilities: $y=\\arg \\max p(y|x)=\\arg \\max p(y,x)$\n",
    "\n",
    "Solve this by dynamic programming. \n",
    "\n",
    "Let $Q_{ts}$ be the most probable sequence of hidden states of length $t$ that finishes in the state $s$ and generate $o_1,...o_T$. Let $q_{t,s}$ be the probability of that sequence. Then, $q_{t,s}$ can be computed dynamically:\n",
    "\n",
    "$q_{t,s}=\\max_{s'}q_{t-1,s'}p(s'|s)p(o_t|s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $p(s'|s)$ are transition probabilities and $p(o_t|s)$ are output probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$q_{t,s}$ can be computed by recalling the argmax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. MEMMs, CRFs and other sequential models for Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Hidden markov models are generative models because they model both, $x$ and $y$. \n",
    "\n",
    "2. Maximum entropy markov Discriminative model: $p(y|x)=\\prod_{t=1}^Tp(y_t|y_{t-1}x_t)$ \n",
    "\n",
    "In this case: $p(y_t|y_{t-1}x_t)=\\frac{1}{Z_t(y_{t-1},x_t)}\\exp\\left(\\sum_{k=1}^K\\theta_kf_k(y_t,y_{t-1},x_t)\\right)$\n",
    "\n",
    "3. CRF: $p(y_t|y_{t-1}x_t)=\\frac{1}{Z_x}\\prod_{t=1}^T\\exp\\left(\\sum_{k=1}^K\\theta_kf_k(y_t,y_{t-1},x_t)\\right)$ -> Faster to run because of the $Z_x$ is outside of the product. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distributed representations. Similar words should have similar vectors. \n",
    "\n",
    "$C^{|V|\\times m}$-> Matrix of distributed word representations. \n",
    "\n",
    "## 1. Probabilistic Neural Language Model\n",
    "\n",
    "$p(w_i|w_{i-n+1},...,w_{i-1})=\\frac{exp(y_{w_i})}{\\sum_{w\\in V}exp(y_{w_i})}$  -> Softmax over components of y. Last thing in the neural language model. \n",
    "\n",
    "$y=b+Wx+Utanh(d+Hx)$  -> Feed forward NN with tons of parameters. Middle of Neural Network.\n",
    "\n",
    "$x=\\left[c(w_{i-n+1},...,w_{i-1})\\right]^T$-> Distributed Representation of our contexts. \n",
    "\n",
    "\n",
    "## 2. Log-bilinear Language Model\n",
    "\n",
    "$p(w_i|w_{i-n+1},...,w_{i-1})=\\frac{exp(\\hat r^Tr_{w_i}+b_{w_i})}{\\sum_{w\\in V}exp(\\hat r^Tr_{w_i}+b_{w_i})}$\n",
    "\n",
    "Representation of word: $r_{w_i}=C(w_i)^T$\n",
    "\n",
    "Representation of context: $\\hat r=\\sum_{k=1}^{n-1}W_kC(w_{i-k})^T$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Recurrent neural networks\n",
    "\n",
    "Extremely powerful for any type of sequence. \n",
    "\n",
    "$h_i=f(Wh_{i-1}+Vx_i+b)$\n",
    "\n",
    "$y_i=Uh_i+\\tilde b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
